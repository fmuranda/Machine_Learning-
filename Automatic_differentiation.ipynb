{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOblEktO9qOuLwiF8z356tt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmuranda/Machine_Learning-/blob/main/Automatic_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eyoov4kdSr0P"
      },
      "outputs": [],
      "source": [
        "#@title Automatic Differentiation (Autograd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "qTF_SWmjVpAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define tensors\n",
        "x = torch.autograd.Variable(torch.Tensor(2), requires_grad=True)\n",
        "y = torch.autograd.Variable(torch.Tensor(1), requires_grad=True)\n",
        "z = torch.autograd.Variable(torch.Tensor(5), requires_grad=True)"
      ],
      "metadata": {
        "id": "StgmsUwJVqJP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#@title Compute gradients\n",
        "# compute a\n",
        "a = x - y\n",
        "\n",
        "# Ensure 'a' has the same size as 'z' for element-wise multiplication.\n",
        "# This pads 'a' with zeros if its size is smaller than 'z'.\n",
        "# This assumes that the intent is for 'a' to be extended to match 'z''s dimension.\n",
        "if a.shape[0] < z.shape[0]:\n",
        "    padding = z.shape[0] - a.shape[0]\n",
        "    a_compatible = F.pad(a, (0, padding), 'constant', 0)\n",
        "elif a.shape[0] > z.shape[0]:\n",
        "    # If 'a' is larger than 'z', you might want to truncate 'a' or handle it differently.\n",
        "    # For this fix, we assume 'z' is the target size or 'a' needs to be padded.\n",
        "    a_compatible = a # Or a_compatible = a[:z.shape[0]] if truncation is desired\n",
        "else:\n",
        "    a_compatible = a # Shapes are already compatible\n",
        "\n",
        "# define the function f\n",
        "f = z * a_compatible\n",
        "print(\"Final value for Function f = \",f)\n",
        "# Compute gradients\n",
        "\n",
        "# Provide a gradient argument for non-scalar output f\n",
        "f.backward(torch.ones_like(f))\n",
        "#Print the gradient value\n",
        "print(\"Gradient value for x:\", x.grad)\n",
        "print(\"Gradient value for y:\", y.grad)\n",
        "print(\"Gradient value for z:\", z.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ljXNAa0WpVC",
        "outputId": "1d02ef69-da28-4e6a-c841-b5f7df39eb91",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final value for Function f =  tensor([-0., -0., 0., 0., 0.], grad_fn=<MulBackward0>)\n",
            "Gradient value for x: tensor([0.0000e+00, 1.3593e-43])\n",
            "Gradient value for y: tensor([-1.3593e-43])\n",
            "Gradient value for z: tensor([-1.4714e-43, -1.4714e-43,  0.0000e+00,  0.0000e+00,  0.0000e+00])\n"
          ]
        }
      ]
    }
  ]
}