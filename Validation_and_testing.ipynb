{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7dDZbbgeWA61Jd/GXdj/h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmuranda/Machine_Learning-/blob/main/Validation_and_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsdY9j56f5g_"
      },
      "outputs": [],
      "source": [
        "#@title Validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import libraries and dataset\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "veB_ppg5gF8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define neural network,_init_ and forward functions\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "u0m_RgplgSi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instantiate the model\n",
        "net = Net()"
      ],
      "metadata": {
        "id": "a4TJQniJmlps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "VxljKZLOm16a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and transform the data\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean = (0.4914, 0.4822, 0.4465),\n",
        "        std = (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True,\n",
        "                       download=True, transform=train_transforms)\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [40000, 10000])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "                    train_set,\n",
        "                    batch_size=4,\n",
        "                    shuffle=True)\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(\n",
        "                    val_set,\n",
        "                    batch_size=4,\n",
        "                    shuffle=False)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "                    train_set,\n",
        "                    batch_size=4,\n",
        "                    shuffle=True,\n",
        "                    num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYdutmtgnqL1",
        "outputId": "be78ddaa-488a-4214-cd6f-a7d5c78d1adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 105MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set the model to training mode and evaluation mode for validation\n",
        "for epoch in range(10):\n",
        "    net.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    net.eval() # Set the model to evaluation mode for validation\n",
        "    validation_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader: # Changed testloader to valloader\n",
        "          images, labels = data\n",
        "          outputs = net(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          validation_loss += loss.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(trainloader)}, Validation Loss: {validation_loss/len(valloader)}\") # Completed the print statement"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rL7d_fNqcME",
        "outputId": "48c520a8-4d1b-4f60-f997-f78710e89755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.7966558238506316, Validation Loss: 1.571040722990036\n",
            "Epoch 2, Training Loss: 1.5197837747365237, Validation Loss: 1.4208078810513018\n",
            "Epoch 3, Training Loss: 1.4236456586308777, Validation Loss: 1.4158243239104749\n",
            "Epoch 4, Training Loss: 1.3661673892620951, Validation Loss: 1.3466906572818755\n",
            "Epoch 5, Training Loss: 1.327568440438807, Validation Loss: 1.3122045918643475\n",
            "Epoch 6, Training Loss: 1.2957131280086935, Validation Loss: 1.3080197643905878\n",
            "Epoch 7, Training Loss: 1.2782197581339627, Validation Loss: 1.2554400429069996\n",
            "Epoch 8, Training Loss: 1.251092935793288, Validation Loss: 1.288907200974226\n",
            "Epoch 9, Training Loss: 1.232424367008172, Validation Loss: 1.2306092447448522\n",
            "Epoch 10, Training Loss: 1.2203342051234096, Validation Loss: 1.2030589907258749\n"
          ]
        }
      ]
    }
  ]
}